{
    "warc" : {
        "title" : "Dataset generator config."
        # Indexing configuration:
        "index" : {
            # What to extract:
            "extract" : {
                # Maximum payload size allowed to be kept wholly in RAM:
                "inMemoryThreshold" : 50M,
                # Maximum payload size that will be serialised out to disk instead of held in RAM:
                "onDiskThreshold" : 1000M,
                # Where in the delivery chain hashing/digest is performed. Valid values are
                # first: The hashing will be calculated over the WARC-payload bytes as-is.
                # after_dechunk_before_decompression: The hashing will be calculated after dechunking, but before decompression.
                # after_dechunk_after_decompression: The hashing will be calculated after dechunking and after decompression.
                # If no hashStage is defined in the config, the default "after_dechunk_before_decompression" will be used.
                "hashStage" : "after_dechunk_before_decompression",
                
                # Content to extract
                "content" : {
                    # Should we index the content body text?
                    "text" : true,
                    
                    # Should we store the content body text?
                    "text_stored" : true,
                    
                    # Extract UK Postcodes and geoindex?
                    "text_extract_postcodes": true,

                    # Run simple AFINN sentiment analysis?                    
                    "test_sentimentj": false,
                     
                    # Run the Stanford NER?
                    "text_stanford_ner": false,
                    
                    # Should we extract the fuzzy hash of the text?
                    "text_fuzzy_hash" : true,
                    
                    # Extract list of elements used in HTML:
                    "elements_used" : true,
                    
                    # Extract potential PDF problems using Apache PDFBox Preflight:
                    "extractApachePreflightErrors" : false,
                    
                    # Extract image features:
                    "images" : {
                        "enabled" : false,
                        "maxSizeInBytes" : 1M,
                    	"detectFaces" : true,
                    	"dominantColours" : true,
                        # The random sampling rate:
                        # (where '1' means 'extract from all images', 
                        # and '100' would mean 'extract from 1 out of every 100 images')
                        "analysisSamplingRate": 50
                    }
                    
                    # Language profiles to load for langdetect
                    "language" : {
                        "enabled" : true,
                        langdetectprofiles : [ "af", "ar", "bg", "bn", "cs", "da", "de", "el", "en", "es", "et", "fa", "fi", "fr", "gu", "he", "hi", "hr", "hu", "id", "it", "ja", "kn", "ko", "lt", "lv", "mk", "ml", "mr", "ne", "nl", "no", "pa", "pl", "pt", "ro", "ru", "sk", "sl", "so", "sq", "sv", "sw", "ta", "te", "th", "tl", "tr", "uk", "ur", "vi", "zh-cn", "zh-tw" ]
                    }

                    # Extract the first bytes of the file (for shingling):
                    "first_bytes" : {
                        # Enabled?
                        "enabled" : true,
                        # Number of bytes to extract (>=4 to allow content_ffb to work):
                        "num_bytes" : 32
                    }

                    # Annotations file
                    "annotations" : {
                        "enabled" : false,
                        "file" : "/path/to/annotations.json",
                        "surt_prefix_file" : "/path/to/openAccessSurts.txt"
                    }

                },
                
                # Which linked entities to extract:
                "linked" : {
                    "normalise" : false,
                    "resources" : true,
                    "hosts" : true,
                    "domains" : true,
                    "images" : true
                },
                
                # Restrict record types:
                "record_type_include" : [
                ],
                
                # Restrict response codes:
                # works by matches starting with the characters, so "2" will match 2xx:
                "response_include" : [
                ],
    
                # Restrict protocols:
                "protocol_include" : [
                    http,
                    https
                ],
   
                # URLs to skip, e.g. ['robots.txt'] ???
                # Matching is done using regexp with implicit pre- and post-fixes '.*'
                # Sample: "url_exclude" becomes ".*url_exclude.*"
                "url_exclude" : []

            },
            
            # Parameters relating to format identification:   
            "id" : {
                # Allow tools to infer format from the resource URI (file extension):
                "useResourceURI" : true
    
                # DROID-specific config:
                "droid" : {
                    "enabled" : true,
                    "useBinarySignaturesOnly" : false
                },
            },
            
            # Parameters to control Apache Tika behaviour
            "tika" : {
                # Maximum length of text to extract:
                "max_text_length" : 512K,
                # Should we use the 'boilerpipe' text extractor?:
                "use_boilerpipe": false,
                # The parse timeout (for when Tika gets stuck):
                "parse_timeout" : 300000,
                # Formats to avoid processing
                "exclude_mime" : [
                    "x-tar",
                    "x-gzip",
                    "bz",
                    "lz",
                    "compress",
                    "zip",
                    "javascript",
                    "css",
                    "octet-stream"
                ],
                # Should we extract EXIF location data:
                "extract_exif_location": true,
                # Should we extract all the available metadata:
                "extract_all_metadata": false,
                # Author field is mono value in Solr schema (optional,default false):
                "author_mono_valued": false                
            },
            
            # Parameters to control the exclusion of results from the indexing process:
            "exclusions" : {
                # Exclusion enabled?
                "enabled" : false,
                # Default check interval before reloading the exclusions file, in seconds:
                "check_interval" : 600,
                # Exclusion URI/SURT prefix file:
                "file" : "/path/to/exclude.txt"
            }
        },
        
        # Solr configuration:
        "solr" : {
            # Use the hash+url as the ID for the documents
            "use_hash_url_id": false
            # Check SOLR for duplicates during indexing:
            "check_solr_for_duplicates": false
            # Server configuration:
            "server" : "http://localhost:8080/solr/discovery",
            # Solr max document batch size in document count for submissions:
            "batch_size" : 500,
            # Solr max document batch size in total bytes for submissions:
            "batch_bytes" : 20M,
            # Is this a dummy-run? (i.e. should we NOT post to SOLR?)
            "dummy_run" : false,
            # Disable explicit commit
            "disablecommit" : false,

            # field-specific setup
            "field_setup" : {
                # The default is used for the content of all SolrFields, unless overridden for any specific field in
                # in field_setup.fields
                "default" : {
                    # The maximum number of values for this field. Extra values are discarded
                    # -1 means no limit (practically 2 billion)
                    # 0 disables the field in Solr. Note: This does not disable the warc-indexer processing overhead for the field content
                    # Default is -1
                    "max_values" : -1,
                    # Text with more that this number of characters is truncated down to the max_length
                    # -1 means no limit (practically 2 billion)
                    # 0 disables the field in Solr. Note: This does not disable the warc-indexer processing overhead for the field content
                    # Default is -1
                    "max_length" : -1,
                    # Custom rewrites of field content. This is effectively a chain of
                    # Pattern.compile(pattern).matcher(content).replaceAll(replacement)
                    "rewrites" : [
#                        {
#                            pattern: "",
#                            replacement: ""
#                        }
                    ],
                    # Ensures that the content can be represented as UTF-8 without exceptions.
                    # History shows that some content using high order characters can trigger the construction of
                    # illegal UTF-8, leading to exceptions during indexing.
                    # Default: true (highly recommended)
                    "sanitize_utf8" : true,
                    # Removes ASCII control characters from the content
                    # Default: true
                    "remove_control_characters" : true,
                    # Remove leading and trailing space characters
                    # Collapse multiple subsequent spaces down to a single space
                    # Default: true
                    "normalise_whitespace" : true
                },
                # Deprecated: Use field_setup.default.max_length instead
                "default_max_length" : -1,
                # keys are field names, values follows the pattern from field_setup.default
                "fields" : {
                    "url" :      {
                        # de facto max length for GET URLs
                        # Default: 2000
                        "max_length" : 2000
                    },
                    "url_norm" : {
                        # de facto max length for GET URLs
                        # Default: 2000
                        "max_length" : 2000,
                        "rewrites" : [
                            {
                                # Heritrix 3.4.0-IIPC-NAS-SNAPSHOT-2019-11-20T10:01:48Z parses img srcset wrong,
                                # attempting to follow and harvest image links such as "http://example.com/foo.jpg 720w".
                                # Some webservers ignore trailing terms and provide the same response as for "http://example.com/foo.jpg".
                                # However, the WARC-Target-URI for the resource is unchanged at "http://example.com/foo.jpg%20720w"
                                # which works poorly for playback.

                                # This rule compensates for that problem so that the indexed URL will be "http://example.com/foo.jpg"
                                pattern: "^(.*)(%20[0-9.]+[wx])$",
                                replacement: "$1"
                            }
                        ]
                    },
                    "links" :    {
                        # de facto max length for GET URLs
                        # Default: 2000
                        "max_length" : 2000
                    },
                    "content" :  {
                        # Same as tika.max_text_length
                        # Default: 512K
                        "max_length" : 512K
                    },
                },
            },
        },

        # Some Hadoop defaults (deprecated):
        "hadoop": {
            "num_reducers": 1
        },
        
        # HTTP Proxy to use when talking to Solr (if any):
        # TODO: This seems to have never been used!? Remove it and log a warning if it is part of a legacy config
        "http_proxy" : {
            #"host" : explorer.bl.uk,
            #"port" : 3127
        }
    }
}
    